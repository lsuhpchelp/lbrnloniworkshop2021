{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deeplearning_R.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.3.1"
    },
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsuhpchelp/lbrnloniworkshop2021/blob/main/day5/deeplearning_R.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB3S7GYJhWMd"
      },
      "source": [
        "Deep learning with R\n",
        "==="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VfF3MZZbBdk"
      },
      "source": [
        "# Outline\n",
        "* Install and load R packages\n",
        "* `keras` package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exyDNnaoLyb9"
      },
      "source": [
        "# 1. Install and load R packages\n",
        "\n",
        "May take a while on the Colab\n",
        "\n",
        "R packages to be installed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqbPQILqLY3e"
      },
      "source": [
        "install.packages(\"keras\")\n",
        "install.packages(\"AmesHousing\")\n",
        "install.packages(\"rsample\")\n",
        "install.packages(\"yardstick\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZUJdm8fhW_P"
      },
      "source": [
        "getwd()\n",
        "list.files()\n",
        "download.file(\"https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip\",\"mnist_csv.zip\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmE_zcyZL6tE"
      },
      "source": [
        "Load R packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHL7BY9kLxSb"
      },
      "source": [
        "library(keras)\n",
        "library(AmesHousing)\n",
        "library(rsample)\n",
        "library(yardstick)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9PYryAIMThp"
      },
      "source": [
        "# 2. `keras` pakcage\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AOuzGDCpd6S"
      },
      "source": [
        "## 2.1 A classification example: `MNIST` data\n",
        "* MNIST (Mixed National Institute of Standards and Technology\n",
        "database) is a large database of handwritten digits that is commonly\n",
        "used for training various image processing systems.\n",
        "* It consists of 70000 images of handwritten digits like these:\n",
        "\n",
        "> https://drive.google.com/open?id=1DnQzdwWLR4EnWuDteA8H9hgK6JOS3vmI\n",
        "\n",
        "* Each image is 28 pixels by 28 pixels\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu81gEx7dteL"
      },
      "source": [
        "### 2.1.1 Data import\n",
        "\n",
        "* `MNIST` data is included in the `keras` package and can be accessed using the `dataset_mnist()` function, which has 60000 training images and 10000 testing images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goEezzOUL12m"
      },
      "source": [
        "mnist <- dataset_mnist()\n",
        "x_train <- mnist$train$x\n",
        "y_train <- mnist$train$y\n",
        "x_test <- mnist$test$x\n",
        "y_test <- mnist$test$y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih-K_JiiPxH6"
      },
      "source": [
        "### 2.1.2 Flatten Operation\n",
        "\n",
        "* Since **each image is 28 pixels by 28 pixels**. We can interpret this as a big array of numbers.\n",
        "\n",
        "* We can flatten this array into a vector of 28x28 = 784 numbers. It doesn't matter how we flatten the array, as long as we're consistent between images.\n",
        "* Then, since the raw data has the grayscale values from integers ranging between 0 to 255, we convert the grayscale values into floating point values ranging between 0 and 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmw_pb5xMLm0"
      },
      "source": [
        "# reshape\n",
        "x_train <- array_reshape(x_train, c(nrow(x_train), 784))\n",
        "x_test <- array_reshape(x_test, c(nrow(x_test), 784))\n",
        "# rescale\n",
        "x_train <- x_train / 255\n",
        "x_test <- x_test / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SVEFEXhREW1"
      },
      "source": [
        "###2.1.3 One-hot encoding response variable `y`\n",
        "* For the purposes of this tutorial, we label the y’s as \"one hot vectors\".\n",
        "* A one hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension.\n",
        "*  For example, how to label an “8”?\n",
        "> * \\[0, 0, 0, 0, 0, 0, 0, 1, 0, 0 ]\n",
        "* The one hot vector can be created using the `to_categorical()` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYwFAkd4MUCK"
      },
      "source": [
        "y_train <- to_categorical(y_train, 10)\n",
        "y_test <- to_categorical(y_test, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWZwCUnFdMBW"
      },
      "source": [
        "## 2.2 Model definition\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2gPykENhaxe"
      },
      "source": [
        "### 2.2.1 A sequential model with two hidden layers\n",
        "* A sequential model can be created by the `keras_model_sequential()` function then a series of layer functions.\n",
        "> * fully-connected layers are added by using the pipe (`%>%`) operator, each layer is defined by the `layer_dense()` function. (\"dense\" means fully-connected) \n",
        "> * **`units`** positive integer, dimensionality of the output space (hidden nodes). The number of hidden nodes in a hidden layer is equal to or less than the number of features in the input but this is not a rule of thumb. \n",
        "> * **`input_shape`** dimensionality of the input (integer) **not** including the samples' axis (samples' axis is 60000 in this example). This argument is required when using this layer as the first layer in a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdrTXLA8MglS"
      },
      "source": [
        "model <- keras_model_sequential() \n",
        "model %>% \n",
        "  layer_dense(units = 256, input_shape = c(784)) %>% # hidden layer 1\n",
        "  layer_dense(units = 10, activation = 'softmax')    # output layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Avi7aFrMlRe"
      },
      "source": [
        "summary(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsEPXAF1uCZr"
      },
      "source": [
        "## Quiz\n",
        "1. How to calculate the total weights (203530) ? \n",
        "2. Why the hidden nodes in the first hidden layer is set to 256? \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CsBuD5Iuprd"
      },
      "source": [
        "### 2.2.2 Compiling the model \n",
        "* Next, configure the model with appropriate loss function, optimizer, and metrics via the `compile` function.\n",
        "> * **`loss = 'categorical_crossentropy'`** remember for classification we use cross-entropy\n",
        "> *  **`optimizer = optimizer_rmsprop()`** call an optimizer function \n",
        "> * For any classification problem you will want to set this to **`metrics = c('accuracy')`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WCyWijXMtY0"
      },
      "source": [
        "model %>% compile(\n",
        "  loss = 'categorical_crossentropy',\n",
        "  optimizer = optimizer_rmsprop(),\n",
        "  metrics = c('accuracy')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTEIAu2YxEnZ"
      },
      "source": [
        "### 2.2.3 Initial model training\n",
        "* The training data can be trained by the `fit()` function.\n",
        "> * **`batch_size`** can be set to a power of 2 (e.g. 32, 64, 128, 256 etc.) to better feed the CPU or GPU hardware. The small values will make the training more computationally intensive while large values provide less feedback signal.\n",
        "> * **`epochs`**  one epoch consists of one full training cycle on the training set. The more complex the features and relationships in your data, the more epochs you will require for your model to learn, adjust the weights, and minimize the loss function.\n",
        "> * **`validation_split`** splits the training dataset into small batches for the cross-validation. The model will hold a small portion of the data (20% in the example below) for the validation purpose (and will not train on it), while the rest data is used to calculate out-of-sample error and update model coefficients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pf4Kj2zMxbu"
      },
      "source": [
        "history <- model %>% fit(\n",
        "  x_train, y_train, \n",
        "  epochs = 30, batch_size = 128, \n",
        "  validation_split = 0.2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqGhk1N5zoE2"
      },
      "source": [
        "* The `history` object returned by the `fit()` function includes loss and accuracy metrics which we can plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EchcJHLNWho"
      },
      "source": [
        "plot(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAAj3na3BUQV"
      },
      "source": [
        "* Use the `evaluate()` function to evaluate the model’s performance on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DluBDTHRNbU0"
      },
      "source": [
        "model %>% evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ihsKCL4BuS-"
      },
      "source": [
        "* Use the `predict_classes()` function to generate predictions on new data,and compare the predicted result to the actual result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8VC7mN4No5s"
      },
      "source": [
        "prediction <- model %>% predict_classes(x_test)\n",
        "results <- data.frame(actual = mnist$test$y, prediction = prediction)\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffd0m4tJOuaR"
      },
      "source": [
        "* Misclassification table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS4btoWcOLMp"
      },
      "source": [
        "attach(results)\n",
        "table(actual,prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHj8_kzcJzw3"
      },
      "source": [
        "## 2.3 Model tuning\n",
        "* The parameters in the initial model can be tuned to find the optimal model. Typically the model tuning follows these steps:\n",
        "> 1. adjust hidden layers and nodes\n",
        "> 2. use a different activation function \n",
        "> 3. increase epochs if the loss function is not flat\n",
        "> 4. add batch normalization\n",
        "> 5. add dropout\n",
        "> 6. add weight regularization\n",
        "> 7. adjust learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfrP1n35nzQ0"
      },
      "source": [
        "### 2.3.1 Hidden layers and nodes adjustment\n",
        "* Hidden layers and nodes are the first of many tuning parameters.\n",
        "* A deep neural network model tends to be overfitted. So how do we examine a model is overfitted or not? \n",
        "> * check the fitting situation on the test data set. If it deteriates then the model is overfitted. \n",
        "\n",
        "An example with three hidden layers and 5000, 1000, and 500 nodes per layer respectively. It took a long time to finish on the Colab so indead of running the following code, I just put the result:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYrzL2HXpM_u"
      },
      "source": [
        "# model <- keras_model_sequential() \n",
        "# model %>% \n",
        "#   layer_dense(units = 5000, input_shape = c(784)) %>% # hidden layer 1\n",
        "#   layer_dense(units = 1000) %>%    \n",
        "#   layer_dense(units = 500) %>%  # hidden layer 2\n",
        "#   layer_dense(units = 10, activation = 'softmax')    # output layer\n",
        "# summary(model)\n",
        "\n",
        "# model %>% compile(\n",
        "#   loss = 'categorical_crossentropy',\n",
        "#   optimizer = optimizer_rmsprop(),\n",
        "#   metrics = c('accuracy')\n",
        "# )\n",
        "\n",
        "# history <- model %>% fit(\n",
        "#   x_train, y_train, \n",
        "#   epochs = 30, batch_size = 128, \n",
        "#   validation_split = 0.2\n",
        "# )\n",
        "\n",
        "\n",
        "# model %>% evaluate(x_test, y_test)\n",
        "\n",
        "#$loss\n",
        "#    14.4901676010132\n",
        "# $acc\n",
        "#     0.101000003516674"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBWgn29hr40n"
      },
      "source": [
        "* Typically, 2-5 hidden layers are sufficient for a  regular rectangular data. \n",
        "\n",
        "We can use 4 hidden layers to see if there is an improvement:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73mE_MQKtSk9"
      },
      "source": [
        "model <- keras_model_sequential() \n",
        "model %>% \n",
        "  layer_dense(units = 256, input_shape = c(784)) %>% # hidden layer 1\n",
        "  layer_dense(units = 128) %>% # hidden layer 2  \n",
        "  layer_dense(units = 64) %>%  # hidden layer 3\n",
        "  layer_dense(units = 32) %>%  # hidden layer 4\n",
        "  layer_dense(units = 10, activation = 'softmax')    # output layer\n",
        "summary(model)\n",
        "\n",
        "model %>% compile(\n",
        "  loss = 'categorical_crossentropy',\n",
        "  optimizer = optimizer_rmsprop(),\n",
        "  metrics = c('accuracy')\n",
        ")\n",
        "\n",
        "history <- model %>% fit(\n",
        "  x_train, y_train, \n",
        "  epochs = 30, batch_size = 128, \n",
        "  validation_split = 0.2\n",
        ")\n",
        "\n",
        "plot(history)\n",
        "model %>% evaluate(x_test, y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmbrKS_gndZM"
      },
      "source": [
        "### 2.3.2 Applying a different activation function\n",
        "* The `activation` argument is for specifying the activation functions.\n",
        "\n",
        "We can try Rectified Linear Unit (ReLU) activation function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7OIboknu63Y"
      },
      "source": [
        "model <- keras_model_sequential() \n",
        "model %>% \n",
        "  layer_dense(units = 256, activation = \"relu\", input_shape = c(784)) %>% # hidden layer 1\n",
        "  layer_dense(units = 128,activation = \"relu\") %>% # hidden layer 2  \n",
        "  layer_dense(units = 64,activation = \"relu\") %>%  # hidden layer 3\n",
        "  layer_dense(units = 32,activation = \"relu\") %>%  # hidden layer 4\n",
        "  layer_dense(units = 10, activation = 'softmax')    # output layer\n",
        "summary(model)\n",
        "\n",
        "model %>% compile(\n",
        "  loss = 'categorical_crossentropy',\n",
        "  optimizer = optimizer_rmsprop(),\n",
        "  metrics = c('accuracy')\n",
        ")\n",
        "\n",
        "history <- model %>% fit(\n",
        "  x_train, y_train, \n",
        "  epochs = 30, batch_size = 128, \n",
        "  validation_split = 0.2\n",
        ")\n",
        "\n",
        "plot(history)\n",
        "\n",
        "model %>% evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3c1-UCYnSt7"
      },
      "source": [
        "### 2.3.3 Epoch adjustment\n",
        "\n",
        "* Increase if the loss function is not flat. \n",
        "* By contrast, if epochs flattens early then there is no need to run so many epochs to save the computational resources. \n",
        "> * The `callback()` function inside the `fit()` is to help with this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ5VuIo77AMC"
      },
      "source": [
        "model <- keras_model_sequential() \n",
        "model %>% \n",
        "  layer_dense(units = 256, activation = \"relu\", input_shape = c(784)) %>% # hidden layer 1\n",
        "  layer_dense(units = 128,activation = \"relu\") %>% # hidden layer 2  \n",
        "  layer_dense(units = 64,activation = \"relu\") %>%  # hidden layer 3\n",
        "  layer_dense(units = 32,activation = \"relu\") %>%  # hidden layer 4\n",
        "  layer_dense(units = 10, activation = 'softmax')    # output layer\n",
        "summary(model)\n",
        "\n",
        "model %>% compile(\n",
        "  loss = 'categorical_crossentropy',\n",
        "  optimizer = optimizer_rmsprop(),\n",
        "  metrics = c('accuracy')\n",
        ")\n",
        "\n",
        "history <- model %>% fit(\n",
        "  x_train, y_train, \n",
        "  epochs = 30, batch_size = 128, \n",
        "  validation_split = 0.2,\n",
        " callbacks = list(callback_early_stopping(patience = 1))\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE_z0uKD1sqL"
      },
      "source": [
        "model %>% evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2XfnGD_iNYT"
      },
      "source": [
        "### 2.3.4 Batch normalization\n",
        "* So far we have normalized our data before feeding it into our model. If the input  is benefiting from it, why not do the same thing also for the values in the hidden layers.\n",
        "* `The layer_batch_normalization() function` normalizes the activations of the previous layer at each batch,  i.e.   applies a transformation that\n",
        "maintains the mean activation close to 0 and the activation standard deviation close to 1 \n",
        "* The main effect of batch normalization is that it helps with gradient propogation, which allows for deeper networks. Consequently, as the depth of your DNN increases, batch normalization becomes more important.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVT5N7Hl-kQ7"
      },
      "source": [
        "model <- keras_model_sequential() \n",
        "model %>% \n",
        "  layer_dense(units = 256, activation = \"relu\", input_shape = c(784)) %>% # hidden layer 1\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dense(units = 128,activation = \"relu\") %>% # hidden layer 2  \n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dense(units = 64,activation = \"relu\") %>%  # hidden layer 3\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dense(units = 32,activation = \"relu\") %>%  # hidden layer 4\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dense(units = 10, activation = 'softmax')    # output layer\n",
        "summary(model)\n",
        "\n",
        "model %>% compile(\n",
        "  loss = 'categorical_crossentropy',\n",
        "  optimizer = optimizer_rmsprop(),\n",
        "  metrics = c('accuracy')\n",
        ")\n",
        "\n",
        "history <- model %>% fit(\n",
        "  x_train, y_train, \n",
        "  epochs = 30, batch_size = 128, \n",
        "  validation_split = 0.2,\n",
        " #callbacks = callback_early_stopping(patience = 2)\n",
        ")\n",
        "\n",
        "plot(history)\n",
        "\n",
        "model %>% evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDNMcKuhjXDt"
      },
      "source": [
        "### 2.3.5 Adding dropout\n",
        "* Dropout is an extremely effective, simple regularization.\n",
        "* Dropout is implemented by removing nodes with some probability p (typically from 0.2 - 0.5) to prevent the model from overfitting.\n",
        "* It is quite simple to apply dropout in `keras` with the  `layer_dropout() `function.\n",
        "\n",
        "Apply a dropout rate 0.25 (drop 25% of the neurons):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N62vYyDjFsrd"
      },
      "source": [
        "model <- keras_model_sequential() \n",
        "model %>% \n",
        "  layer_dense(units = 256, activation = \"relu\", input_shape = c(784)) %>% # hidden layer 1\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dropout(rate = 0.25) %>%\n",
        "  layer_dense(units = 128,activation = \"relu\") %>% # hidden layer 2  \n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dropout(rate = 0.25) %>%\n",
        "  layer_dense(units = 64,activation = \"relu\") %>%  # hidden layer 3\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dropout(rate = 0.25) %>%\n",
        "  layer_dense(units = 32,activation = \"relu\") %>%  # hidden layer 4\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dropout(rate = 0.25) %>%\n",
        "  layer_dense(units = 10, activation = 'softmax')    # output layer\n",
        "summary(model)\n",
        "\n",
        "model %>% compile(\n",
        "  loss = 'categorical_crossentropy',\n",
        "  optimizer = optimizer_rmsprop(),\n",
        "  metrics = c('accuracy')\n",
        ")\n",
        "\n",
        "history <- model %>% fit(\n",
        "  x_train, y_train, \n",
        "  epochs = 30, batch_size = 128, \n",
        "  validation_split = 0.2,\n",
        ")\n",
        "\n",
        "plot(history)\n",
        "\n",
        "model %>% evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkjJxcYekUeK"
      },
      "source": [
        "### 2.3.6 Adding weight regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPwSaoSZkydt"
      },
      "source": [
        "* Remember we put a penalty such as weight decay to obtian a more stable fit when using the `nnet` package. \n",
        "* Weight decay sacrifices some fit to the current data to obtain a more stable result.\n",
        "* The `keras` package also provides weight decay regularization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uWyofL5IuSX"
      },
      "source": [
        "model <- keras_model_sequential() \n",
        "model %>% \n",
        "  layer_dense(units = 256, activation = \"relu\", input_shape = c(784),kernel_regularizer = regularizer_l2(0.001)) %>% # hidden layer 1\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dense(units = 128,activation = \"relu\", kernel_regularizer = regularizer_l2(0.001)) %>% # hidden layer 2  \n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dense(units = 64,activation = \"relu\", kernel_regularizer = regularizer_l2(0.001)) %>%  # hidden layer 3\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dense(units = 32,activation = \"relu\", kernel_regularizer = regularizer_l2(0.001)) %>%  # hidden layer 4\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dense(units = 10, activation = 'softmax')    # output layer\n",
        "summary(model)\n",
        "\n",
        "model %>% compile(\n",
        "  loss = 'categorical_crossentropy',\n",
        "  optimizer = optimizer_rmsprop(),\n",
        "  metrics = c('accuracy')\n",
        ")\n",
        "\n",
        "history <- model %>% fit(\n",
        "  x_train, y_train, \n",
        "  epochs = 30, batch_size = 128, \n",
        "  validation_split = 0.2,\n",
        ")\n",
        "\n",
        "plot(history)\n",
        "\n",
        "model %>% evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_C2PcQplUpW"
      },
      "source": [
        "### 2.3.7 Learning rate adjustment\n",
        "* We can automatically adjust the learning rate by a factor of 2-10 once the validation loss has stopped improving.\n",
        "\n",
        "`callback_reduce_lr_on_plateau()` will divide the learning rate by 10 if the validation loss value does not improve for 10 epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JcRosDbK9_f"
      },
      "source": [
        "model <- keras_model_sequential() \n",
        "model %>% \n",
        "  layer_dense(units = 256, activation = \"relu\", input_shape = c(784)) %>% # hidden layer 1\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dropout(rate = 0.25) %>%\n",
        "  layer_dense(units = 128,activation = \"relu\") %>% # hidden layer 2  \n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dropout(rate = 0.25) %>%\n",
        "  layer_dense(units = 64,activation = \"relu\") %>%  # hidden layer 3\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dropout(rate = 0.25) %>%\n",
        "  layer_dense(units = 32,activation = \"relu\") %>%  # hidden layer 4\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dropout(rate = 0.25) %>%\n",
        "  layer_dense(units = 10, activation = 'softmax')    # output layer\n",
        "summary(model)\n",
        "\n",
        "model %>% compile(\n",
        "  loss = 'categorical_crossentropy',\n",
        "  optimizer = optimizer_rmsprop(),\n",
        "  metrics = c('accuracy')\n",
        ")\n",
        "\n",
        "history <- model %>% fit(\n",
        "  x_train, y_train, \n",
        "  epochs = 30, batch_size = 128, \n",
        "  validation_split = 0.2,\n",
        "  callbacks = list(callback_reduce_lr_on_plateau(factor = 0.1, patience = 10))\n",
        ")\n",
        "\n",
        "plot(history)\n",
        "\n",
        "model %>% evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEqAYzpC2rCR"
      },
      "source": [
        "## Quiz\n",
        "* Are we missing something important about the data since the very beginning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWPXHMiJaS-v"
      },
      "source": [
        "## 2.4 Convolution Neuron Network (CNN)\n",
        "* CNN are one of the most popular deep neural networks for visual imagery analysis. \n",
        "* CNN takes advantage of 2D image structure.\n",
        "* In the same convolutional layer, each neuron only \"sees\" a small part of the image (or layer) above it, and is using the same weights and bias.\n",
        "* The infomation from the convolutional layer will be simplified by pooling layer immediately. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB8In5zl2NoA"
      },
      "source": [
        "### 2.4.1 Data preparation\n",
        "* Import and reshape data \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyHbWE1B2UMF"
      },
      "source": [
        "mnist <- dataset_mnist()\n",
        "x_train <- mnist$train$x\n",
        "y_train <- mnist$train$y\n",
        "x_test <- mnist$test$x\n",
        "y_test <- mnist$test$y\n",
        "\n",
        "\n",
        "# Input image dimensions\n",
        "img_rows <- 28\n",
        "img_cols <- 28\n",
        "\n",
        "# Redefine  dimension of train/test inputs\n",
        "x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))\n",
        "x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))\n",
        "input_shape <- c(img_rows, img_cols, 1)\n",
        "\n",
        "x_train <- x_train / 255\n",
        "x_test <- x_test / 255\n",
        "\n",
        "cat('x_train_shape:', dim(x_train), '\\n')\n",
        "cat(nrow(x_train), 'train samples\\n')\n",
        "cat(nrow(x_test), 'test samples\\n')\n",
        "\n",
        "# One-hot encoding y\n",
        "y_train <- to_categorical(y_train, 10)\n",
        "y_test <- to_categorical(y_test, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Irslgcsx41k8"
      },
      "source": [
        "### 2.4.2 Model definition \n",
        "* code is obtained from https://keras.rstudio.com\n",
        "* convolutional layer is defined by the `layer_conv_2d` function\n",
        "* pooling layer is defined by the `layer_max_pooling_2d()` function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO0yP0A67lar"
      },
      "source": [
        "model <- keras_model_sequential() %>%\n",
        "  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',\n",
        "                input_shape = input_shape) %>% \n",
        "  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>% \n",
        "  layer_max_pooling_2d(pool_size = c(2, 2)) %>% # strides will default to pool_size\n",
        "  layer_dropout(rate = 0.25) %>% \n",
        "  layer_flatten() %>% \n",
        "  layer_dense(units = 128, activation = 'relu') %>% \n",
        "  layer_dropout(rate = 0.5) %>% \n",
        "  layer_dense(units = 10, activation = 'softmax')\n",
        "\n",
        "# Compile model\n",
        "model %>% compile(\n",
        "  loss = loss_categorical_crossentropy,\n",
        "  optimizer = optimizer_adadelta(),\n",
        "  metrics = c('accuracy')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-nKhIFK8pYT"
      },
      "source": [
        "### 2.4.3 Model training\n",
        "* The training process will take ~ 28 mins (a speed of  ~ 140 seconds per epoch) on the Colab. \n",
        "* The R kernel in the Colab doesn't support GPU at this time.  The same code will be run at 16 seconds per epoch on a GRID K520 GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7U-Dw628prn"
      },
      "source": [
        "model %>% fit(\n",
        "  x_train, y_train,\n",
        "  batch_size = 128,\n",
        "  epochs = 12,\n",
        "  validation_split = 0.2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y__-H7LL-5ru"
      },
      "source": [
        "* Challenging 99% test accuracy. There is still a large margin for parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oze9UcUE-4i0"
      },
      "source": [
        "model %>% evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCpHaffINObb"
      },
      "source": [
        "# Getting Help\n",
        "\n",
        "* Documentation: http://www.hpc.lsu.edu/docs\n",
        "* Contact us\n",
        "> * Email ticket system: sys-help@loni.org\n",
        "> * Telephone Help Desk: 225-578-0900\n"
      ]
    }
  ]
}